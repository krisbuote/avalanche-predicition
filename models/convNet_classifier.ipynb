{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 285 images belonging to 5 classes.\n",
      "Found 62 images belonging to 5 classes.\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 78s 5s/step - loss: 2.3265 - acc: 0.3897 - val_loss: 1.2883 - val_acc: 0.3125\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 75s 4s/step - loss: 1.2438 - acc: 0.3748 - val_loss: 1.0313 - val_acc: 0.5217\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 88s 5s/step - loss: 1.2310 - acc: 0.4471 - val_loss: 1.1498 - val_acc: 0.2609\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 100s 6s/step - loss: 1.2619 - acc: 0.4115 - val_loss: 1.0473 - val_acc: 0.5217\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 104s 6s/step - loss: 1.2331 - acc: 0.3740 - val_loss: 1.1499 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 113s 7s/step - loss: 1.2047 - acc: 0.3858 - val_loss: 1.0505 - val_acc: 0.5217\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 88s 5s/step - loss: 1.2084 - acc: 0.4389 - val_loss: 1.2068 - val_acc: 0.3043\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 91s 5s/step - loss: 1.1218 - acc: 0.4154 - val_loss: 1.0429 - val_acc: 0.5652\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 101s 6s/step - loss: 1.1592 - acc: 0.4520 - val_loss: 1.0992 - val_acc: 0.5000\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 91s 5s/step - loss: 1.1611 - acc: 0.4874 - val_loss: 1.0132 - val_acc: 0.5217\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 91s 5s/step - loss: 1.2005 - acc: 0.3860 - val_loss: 1.1751 - val_acc: 0.4130\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 90s 5s/step - loss: 1.1367 - acc: 0.4683 - val_loss: 1.0833 - val_acc: 0.5652\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 91s 5s/step - loss: 1.1878 - acc: 0.4360 - val_loss: 1.1468 - val_acc: 0.4583\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 90s 5s/step - loss: 1.1181 - acc: 0.4352 - val_loss: 1.0199 - val_acc: 0.5217\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 89s 5s/step - loss: 1.1505 - acc: 0.4263 - val_loss: 1.1162 - val_acc: 0.4130\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 87s 5s/step - loss: 1.0930 - acc: 0.4397 - val_loss: 1.0868 - val_acc: 0.5000\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 86s 5s/step - loss: 1.1259 - acc: 0.4536 - val_loss: 1.0817 - val_acc: 0.4583\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 86s 5s/step - loss: 1.1257 - acc: 0.4279 - val_loss: 1.0733 - val_acc: 0.4783\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 86s 5s/step - loss: 1.0779 - acc: 0.4485 - val_loss: 1.0351 - val_acc: 0.5217\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 86s 5s/step - loss: 1.1337 - acc: 0.4542 - val_loss: 1.0613 - val_acc: 0.5435\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 96s 6s/step - loss: 1.0809 - acc: 0.4301 - val_loss: 1.0433 - val_acc: 0.3958\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 86s 5s/step - loss: 1.1150 - acc: 0.5179 - val_loss: 1.1110 - val_acc: 0.3261\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 84s 5s/step - loss: 1.0526 - acc: 0.4794 - val_loss: 1.1040 - val_acc: 0.2826\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 84s 5s/step - loss: 1.1347 - acc: 0.4471 - val_loss: 0.9759 - val_acc: 0.4783\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 84s 5s/step - loss: 1.0491 - acc: 0.5341 - val_loss: 0.9962 - val_acc: 0.4792\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 85s 5s/step - loss: 0.9964 - acc: 0.4998 - val_loss: 0.9443 - val_acc: 0.6304\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 85s 5s/step - loss: 1.0591 - acc: 0.4706 - val_loss: 1.0040 - val_acc: 0.6087\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 84s 5s/step - loss: 1.0311 - acc: 0.5480 - val_loss: 1.0201 - val_acc: 0.4130\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 84s 5s/step - loss: 0.9677 - acc: 0.5235 - val_loss: 1.0000 - val_acc: 0.5625\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 84s 5s/step - loss: 1.0351 - acc: 0.5067 - val_loss: 0.8673 - val_acc: 0.6957\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 83s 5s/step - loss: 0.9979 - acc: 0.5329 - val_loss: 1.0524 - val_acc: 0.4348\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 83s 5s/step - loss: 1.0185 - acc: 0.5517 - val_loss: 1.0044 - val_acc: 0.5217\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 84s 5s/step - loss: 0.9443 - acc: 0.5558 - val_loss: 0.9346 - val_acc: 0.5625\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 83s 5s/step - loss: 1.0418 - acc: 0.4978 - val_loss: 1.0689 - val_acc: 0.5000\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 83s 5s/step - loss: 1.0579 - acc: 0.4920 - val_loss: 0.8971 - val_acc: 0.5435\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 83s 5s/step - loss: 0.9947 - acc: 0.5178 - val_loss: 1.0340 - val_acc: 0.5217\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 82s 5s/step - loss: 1.0342 - acc: 0.4896 - val_loss: 0.9831 - val_acc: 0.5833\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 84s 5s/step - loss: 0.9997 - acc: 0.5043 - val_loss: 0.9750 - val_acc: 0.5000\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 186s 11s/step - loss: 0.9366 - acc: 0.5294 - val_loss: 1.0292 - val_acc: 0.5217\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 227s 13s/step - loss: 1.0264 - acc: 0.5198 - val_loss: 0.9581 - val_acc: 0.5435\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 226s 13s/step - loss: 0.9749 - acc: 0.5663 - val_loss: 0.9841 - val_acc: 0.5625\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 236s 14s/step - loss: 0.9354 - acc: 0.5120 - val_loss: 1.0559 - val_acc: 0.5217\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 158s 9s/step - loss: 0.9618 - acc: 0.4851 - val_loss: 1.0927 - val_acc: 0.4783\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 74s 4s/step - loss: 1.0071 - acc: 0.5353 - val_loss: 0.9589 - val_acc: 0.5652\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 76s 4s/step - loss: 0.9211 - acc: 0.5713 - val_loss: 1.0889 - val_acc: 0.3750\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 78s 5s/step - loss: 1.0534 - acc: 0.4559 - val_loss: 1.0051 - val_acc: 0.6739\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 79s 5s/step - loss: 1.0109 - acc: 0.4491 - val_loss: 0.9595 - val_acc: 0.5000\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 82s 5s/step - loss: 0.9748 - acc: 0.5162 - val_loss: 0.9780 - val_acc: 0.5435\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 87s 5s/step - loss: 0.9620 - acc: 0.5509 - val_loss: 0.9919 - val_acc: 0.5833\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 85s 5s/step - loss: 0.9789 - acc: 0.5125 - val_loss: 0.9344 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate code from https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d\n",
    "# Documentation: https://keras.io/preprocessing/image/\n",
    "# Adapted by Kris Buote for Avalanche prediction\n",
    "\n",
    "# The Keras image generator takes existing images and uses the subdirectories as classes. \n",
    "# It creates a larger data set by doing real-time data augmentation.\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import os, os.path\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 432, 288\n",
    "\n",
    "train_data_dir = '../data/temp_prec_images/Below Treeline/train/'\n",
    "validation_data_dir = '../data/temp_prec_images/Below Treeline/validation'\n",
    "\n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_data_dir)]) # count the number of training images\n",
    "nb_validation_samples = sum([len(files) for r, d, files in os.walk(validation_data_dir)])\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5)) # 5 neuron outputs danger rating prediction vector. i.e. [0, 0, 1, 0, 0] means rating 3\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=False) # consider horizontal_flip = True. I think it would just be \"mirrored in time\"\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
