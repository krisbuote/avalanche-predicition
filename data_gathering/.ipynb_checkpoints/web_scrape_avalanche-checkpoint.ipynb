{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based off of this tutorial: http://stanford.edu/~mgorkove/cgi-bin/rpython_tutorials/Scraping_a_Webpage_Rendered_by_Javascript_Using_Python.php\n",
    "#one year of data at a time -- often get \"OSError: [Errno 24] Too many open files\" if I try doing all at once \n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "saveIt = True\n",
    "\n",
    "def find_substring(substring, string):\n",
    "    \"\"\" \n",
    "    Returns list of indices where substring begins in string\n",
    "\n",
    "    >>> find_substring('me', \"The cat says meow, meow\")\n",
    "    [13, 19]\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    index = -1  # Begin at -1 so index + 1 is 0\n",
    "    while True:\n",
    "        # Find next index of substring, by starting search from index + 1\n",
    "        index = string.find(substring, index + 1)\n",
    "        if index == -1:  \n",
    "            break  # All occurrences have been found\n",
    "        indices.append(index)\n",
    "    return indices\n",
    "\n",
    "#first, get list of dates to use in URLs\n",
    "startYear = 2018 #change this to the year you want\n",
    "finalYear = startYear\n",
    "\n",
    "dates = []\n",
    "for year in range(startYear,finalYear+1):\n",
    "\n",
    "    startDate = str(year)+'-01-01' #YYYY-MM-DD\n",
    "    finalDate = str(year)+'-03-31'\n",
    "\n",
    "    d1 = date(int(startDate[0:4]), int(startDate[5:7]), int(startDate[8:10]))  # start date\n",
    "    d2 = date(int(finalDate[0:4]), int(finalDate[6]), int(finalDate[8:10]))  # end date\n",
    "\n",
    "    delta = d2 - d1\n",
    "    dates = dates+[str(d1+timedelta(i)) for i in range(delta.days+1)] #list of dates (strings) during time range of interest\n",
    "  \n",
    "#now, get list of urls\n",
    "urls = []\n",
    "url_base = 'https://www.avalanche.ca/forecasts/archives/sea-to-sky/'\n",
    "for day in dates:\n",
    "    urls.append(url_base+day)\n",
    "\n",
    "#loop through each URL and scrape \n",
    "columns = ['Below Treeline','Treeline','Above Treeline']\n",
    "index = dates\n",
    "riskStr = pd.DataFrame(index=index,columns = columns)\n",
    "risk = pd.DataFrame(index=index,columns = columns)\n",
    "for url in urls:\n",
    "    \n",
    "    day = url[-10:]\n",
    "    print(day)\n",
    "    \n",
    "    options = webdriver.ChromeOptions();\n",
    "    options.add_argument('headless')\n",
    "    #self.driver = webdriver.Chrome(options=options)\n",
    "    browser = webdriver.Chrome('/Users/samanderson/Downloads/chromedriver',options=options) #since I'm using Chrome -- also had to download 'chromedriver.exe'\n",
    "    browser.get(url)\n",
    "    time.sleep(8) #have to wait while the page loads -- takes a hot minute since it's a lot of javascript animations etc\n",
    "\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\") #returns the inner HTML as a string\n",
    "    html = BeautifulSoup(innerHTML,'html.parser') #makes it prettier\n",
    "    webtxt = html.get_text() #just want the text for easy access to info\n",
    "\n",
    "    inds1Low = find_substring('1 - Low',webtxt) #indices of wherever '1 - Low' appears\n",
    "    inds2Mod = find_substring('2 - Moderate',webtxt) #etc\n",
    "    inds3Con = find_substring('3 - Considerable',webtxt)\n",
    "    inds4High = find_substring('4 - High',webtxt)\n",
    "\n",
    "    #we want to find three smallest indices, since the first three mentions of rating are for the current day's forecast\n",
    "    indsRatings = inds1Low + inds2Mod + inds3Con + inds4High \n",
    "    indsRatings.sort()\n",
    "    firstThreeInds = indsRatings[0:3]\n",
    "    firstThreeRatingsStr = []\n",
    "    firstThreeRatings = []\n",
    "\n",
    "    for ind in firstThreeInds: #this is bulky but it gets it done -- finds the three ratings for current day's below treeline/treeline/above treeline\n",
    "    \n",
    "        try:\n",
    "            inds1Low.index(ind)\n",
    "            firstThreeRatingsStr.append('1 - Low')\n",
    "            firstThreeRatings.append(1)\n",
    "        except:\n",
    "            a = 1 #do nothing\n",
    "        \n",
    "        try:\n",
    "            inds2Mod.index(ind)\n",
    "            firstThreeRatingsStr.append('2 - Moderate')\n",
    "            firstThreeRatings.append(2)\n",
    "        except:\n",
    "            a = 1 #do nothing\n",
    "\n",
    "        try:\n",
    "            inds3Con.index(ind)\n",
    "            firstThreeRatingsStr.append('3 - Considerable')\n",
    "            firstThreeRatings.append(3)\n",
    "        except:\n",
    "            a = 1 #do nothing\n",
    "        \n",
    "        try:\n",
    "            inds4High.index(ind)\n",
    "            firstThreeRatingsStr.append('4 - High')\n",
    "            firstThreeRatings.append(4)\n",
    "        except:\n",
    "            a = 1 #do nothing\n",
    "\n",
    "    browser.close()\n",
    "    riskStr.loc[day]=firstThreeRatingsStr\n",
    "    risk.loc[day]=firstThreeRatings\n",
    "\n",
    "if saveIt is True:\n",
    "    risk.to_csv('/Users/samanderson/avalanche_rating_Jan_to_Mar_'+str(startYear))\n",
    "    riskStr.to_csv('/Users/samanderson/avalanche_rating_string_Jan_to_Mar_'+str(startYear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This opens each year's risk values and concatenates them into one csv file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "saveIt = True #if you want to save the concatenated file with all years\n",
    "\n",
    "startYear = 2012\n",
    "finalYear = 2017\n",
    "\n",
    "csv_name_base = '/Users/samanderson/avalanche_rating_Jan_to_Mar_'\n",
    "\n",
    "data2012 = pd.read_csv(csv_name_base+str(2012),index_col='Unnamed: 0')\n",
    "data2013 = pd.read_csv(csv_name_base+str(2013),index_col='Unnamed: 0')\n",
    "data2014 = pd.read_csv(csv_name_base+str(2014),index_col='Unnamed: 0')\n",
    "data2015 = pd.read_csv(csv_name_base+str(2015),index_col='Unnamed: 0')\n",
    "data2016 = pd.read_csv(csv_name_base+str(2016),index_col='Unnamed: 0')\n",
    "data2017 = pd.read_csv(csv_name_base+str(2017),index_col='Unnamed: 0')\n",
    "\n",
    "data = [data2012,data2013,data2014,data2015,data2016,data2017]\n",
    "\n",
    "risk_all_years = pd.concat(data)\n",
    "\n",
    "if saveIt is True:\n",
    "    risk_all_years.to_csv('/Users/samanderson/avalanche_rating_Jan_to_Mar_2012_to_2017.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
